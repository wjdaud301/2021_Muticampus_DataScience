## RDD 예제 - 단일 데이터



### 단일 데이터를 이용한 분석

---



#### 합계

---

합계를 구하는 방법은 `reduce`함수를 이용해서 처리할 수 있습니다. `reduce`함수에는 데이터가 순서대로 들어오게  됩니다. 순서대로 들어오는 데이터를 모두 더해주면 합계를 구할 수 있습니다.

```scala
val data = Array(1, 2, 3, 4, 5)
val rdd = sc.parallelize(data)

// 같은 결과를 출력 
rdd.reduce(_ + _)
rdd.reduce((x, y) => x + y)
```



#### 평균

---

평균은 합계를 구해서 모든 데이터의 개수로 나누어 주면 됩니다. 처음 입력받은 데이터의 개수로 나누어도  구할 수 있지만 여기서는 `map`,`reduce`함수를 이용해서 구하겠습니다.

데이터를 `map`을 이용해서 (1,값) 의 형태로 바꾸고, `reduce`를 이용해서 (키의 합, 값의 합)을 구하게 되면 키의 합은 전체 데이터의 개수가 되고,  값의 합은 전체 데이터의 합이 됩니다. 이를 값의 합/ 키의 합으로 계산해주면 평균값이 됩니다.

```scala
val data = Array(1, 2, 3, 4, 5)
val rdd = sc.parallelize(data)
val sum = rdd.map(x => (1, x)).reduce((x, y) => (x._1 + y._1, x._2 + y._2))
val mod = sum._2 / sum._1

scala> val mod = sum._2 / sum._1
mod: Int = 3
```





#### 필터링

---

필터링은 `filter`를 이용하여 간단하게 처리할 수 있습니다.

```scala
val data = Array(1, 2, 3, 4, 5)
val rdd = sc.parallelize(data)

// 2보다 큰 데이터만 출력 필터링
rdd.filter(_ > 2).collect()
res17: Array[Int] = Array(3, 4, 5)  
```





#### 데이터 집합

---

RDD 데이터는 집합의 교집합(`intersection`), 차집합(`subtract`), 합계(`union`), 카테시안 곱(`cartesian`) 처리가 가능합니다.

```scala
val data1 = Array(1, 2, 3, 4, 5)
val rdd1 = sc.parallelize(data1)

val data2 = Array(3, 4, 5, 6, 7)
val rdd2 = sc.parallelize(data2)

scala> rdd1.union(rdd2).collect()
res19: Array[Int] = Array(1, 2, 3, 4, 5, 3, 4, 5, 6, 7)                         

scala> rdd1.intersection(rdd2).collect()
res20: Array[Int] = Array(4, 3, 5)

scala> rdd1.subtract(rdd2).collect()
res21: Array[Int] = Array(2, 1)

scala> rdd1.cartesian(rdd2).collect()
res22: Array[(Int, Int)] = Array((1,3), (1,4), (2,3), (2,4), (1,5), (1,6), (1,7), (2,5), (2,6), (2,7), (3,3), (3,4), (4,3), (4,4), (5,3), (5,4), (3,5), (3,6), (3,7), (4,5), (4,6), (4,7), (5,5), (5,6), (5,7))
```





---





## RDD 예제 - 키,밸류 쌍



### 키, 밸류 쌍을 이용한 분석

---



#### 워드 카운트

---

1. 단어가 등장하는 워드 카운트는 `map`을 이용하여 단어를 (키, 등장횟수)로 분리하고, 
2. `reduceByKey`를 이용하여 그룹화하여 개수의 합계를 구하면 됩니다.
3. 키를 기준으로 정렬할 때 `sortByKey`를 이용하여 처리할 수 있습니다.

```scala
val wordsRDD = sc.parallelize(Array("a", "b", "c", "d", "a", "a", "b", "b", "c", "d"))

val pairs = wordsRDD.map(w => (w, 1))   // (단어, 1)
val counts = pairs.reduceByKey(_ + _)   // 단어를 기준으로 개수의 합계 구함
val sorted = counts.sortByKey()     // 키를 기준으로 정렬 
```





#### 워드 카운트 개수를 기준으로 정렬

---

워드카운트 후 개수를 기준으로 정렬할 때는 `sortBy`를 이용하여 처리할 수 있습니다. 오름차순 정렬과 내림차순 정렬에 대한 정보도 전달할 수 있습니다. 

```scala
val wordsRDD = sc.parallelize(Array("a", "b", "c", "d", "a", "a", "b", "b", "c", "d", "d", "d", "d"))
val pairs = wordsRDD.map(w => (w, 1))
val counts = pairs.reduceByKey(_ + _)

// 오름차순 정렬
val sortedV = counts.sortBy(v => v._2)
val sortedV = counts.sortBy(v => v._2, true)

// 다음의 두가지 방법은 같은 결과를 처리(내림차순 정렬)
val sortedV = counts.sortBy(v => v._2, false)
val sortedV = counts.sortBy({ case (w, c) => c } , false)
```





#### 필터링

---

스칼라의 `case` 문법을 이용하여 변수명으로 접근할 수도 있습니다

```scala
val wordsRDD = sc.parallelize(Array("a", "b", "c", "d", "a", "a", "b", "b", "c", "d", "d", "d", "d"))
val pairs = wordsRDD.map(w => (w, 1))
val counts = pairs.reduceByKey(_ + _)

val filtered = counts.filter { case(k, v) => v > 3 }    // value가 3 초과인 데이터 
val filtered = counts.filter { case(k, v) => k == "a" } // key가 a인 데이터 

scala> filtered.collect()
res40: Array[(String, Int)] = Array((d,5))   

scala> filtered.collect()
res46: Array[(String, Int)] = Array((a,3))    
```





#### 키, 밸류에서 밸류에만 처리를 할 경우

---

키, 밸류 쌍에서 밸류에만 어떠한 처리를 하고자 하는 경우에는 `mapvalues`를 이용하여 간결하게 처리할 수 있습니다.

```scala
val wordsRDD = sc.parallelize(Array("a", "b", "c", "d", "a", "a", "b", "b", "c", "d", "d", "d", "d"))
val pairs = wordsRDD.map(w => (w, 1))
val counts = pairs.reduceByKey(_ + _)

val added = counts.mapValues(x => x * x)    // value 만 들어오는 처리 
scala> added.collect()
res44: Array[(String, Int)] = Array((d,25), (b,9), (a,9), (c,4))   

val added = counts.map({ case (k, v) => (k, v*v) }) // map을 이용하여 동일한 처리 
scala> added.collect()
res44: Array[(String, Int)] = Array((d,25), (b,9), (a,9), (c,4))    

```





#### 파티셔닝 

---

데이터 파티셔닝 처리를 위해서는 `partitionBy`를 이용해서 파티셔너를 지정해야 합니다. 사용자가 파티셔너를 따로 구현하여 적용할 수도 있습니다.

```scala
// 해쉬 파티셔너 적용을 위해 import 필요함 
import org.apache.spark.HashPartitioner

val pairs = wordsRDD.map(w => (w, 1)).partitionBy(new HashPartitioner(100)).persist()
val counts = pairs.reduceByKey(_ + _)
val added = counts.mapValues(x => x * x)

scala> pairs.partitioner
res45: Option[org.apache.spark.Partitioner] = Some(org.apache.spark.HashPartitioner@64)
```





#### 조인

---

단일 데이터 처리와 동일하게 `union`, `join`을 이용하여 데이터의 조인을 처리할 수 있습니다. 키, 밸류는 키를 기준으로 조인을 처리하게 됩니다.

```scala
val wordsRDD = sc.parallelize(Array("a", "b", "c", "d", "a", "a", "b", "b", "c", "d", "d", "d", "d"))
val pairs = wordsRDD.map(w => (w, 1))
val counts = pairs.reduceByKey(_ + _)

val filteredA = counts.filter { case(k, v) => k == "b" }
val filteredB = counts.filter { case(k, v) => k == "a" }

filteredA.collect()
filteredB.collect()

val unionRDD = filteredA.union(filteredB)
val joinRDD = unionRDD.join(filteredB)


scala> unionRDD.collect()
res55: Array[(String, Int)] = Array((a,3), (b,3))

scala> joinRDD.collect()
res56: Array[(String, (Int, Int))] = Array((a,(3,3)))
```



