## 1. 스파크 SQL



스파크는 하이브 메타스토어를 연결하여 사용할 수 있습니다. 메타스토어의 데이터베이스, 테이블 정보를 이용하여 작업을 진행할 수 있습니다. RDD, 데이터프레임, 스파크 SQL을 이용하여 작업을 할 수 있습니다.

스파크 SQL은 하이브의 쿼리를 스파크에서 그대로 이용할 수 있게 만들어 줍니다. 그리고 스파크 SQL을 이용하여 조회한 정보는 데이터프레임으로 반환되어, 데이터셋으로 변환하여 작업을 할 수도 있습니다. 스칼라를 이용하여 UDF를 생성할 수도 있고, 기존에 자바로 작성한 UDF를 그대로 이용할 수 도 있습니다.

스파크 SQL을 이용한 작업과 데이터프레임을 이용한 작업은 내부적으로 동일한 최적화 과정을 거쳐서 동작합니다. 1G의 데이터를 이용하여 데이터를 조회하고 그룹핑하여 건수를 조회하는 작업을 처리한 결과 15초로 동일한 작업시간을 보여주었습니다.



### 하이브 메타스토어 연결

---

스파크 세션을 초기화할 때 `hive.metastord.uris`정보를 제공하여 하이브 메타스토어와 연결합니다.
`sql`을 이용하여 스파크 SQL을 처리할 수 있습니다.

```scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder().appName("sample").config("hive.metastore.uris", "thrift://hive_metastore_ip:hive_metastore_port").enableHiveSupport().getOrCreate()

// 스칼라 세션을 이용한 조회 
spark.sql("SELECT * FROM temp")
// 스칼라 쉘(scala-shell 에서 이용 방법)
sql("SELECT * FROM temp")
```





### 스파크 SQL 쿼리

---

스파크 SQL은 하이브의 people 테이블을 이용하여 확인해 보겠습니다. people 테이블은 name, age 칼럼을 가지는 테스트용 테이블입니다.

```
hive> select * from people;
OK
david   15
paul    32
smith   40
jin 27
ryan    46
```



### 조회

---

데이터 조회는 SELECT를 이용합니다. `sql`을 이용하여 조회를 하면 **데이터프레임**이 반환되는 것을 확인할 수 있습니다.

```scala
scala> sql("select * from people")
res19: org.apache.spark.sql.DataFrame = [name: string, age: int]

scala> sql("select * from people").show()
+-----+---+                                                                     
| name|age|
+-----+---+
|david| 15|
| paul| 32|
|smith| 40|
|  jin| 27|
| ryan| 46|
+-----+---+
```





### 조인

---

조인은 SQL 을 이용하여 조인을 할 수도 있고, 데이터프레임의 조인 명령을 이용할 수도 있습니다.



#### SQL을 이용한 조인

표준 SQL의 조인 명령을 그대로 사용할 수 있습니다.

```scala
scala> sql("""SELECT a.name, b.age 
     |          FROM people a, people b
     |         WHERE a.name = b.name""").show()
+-----+---+                                                                     
| name|age|
+-----+---+
|david| 15|
| paul| 32|
|smith| 40|
|  jin| 27|
| ryan| 46|
+-----+---+
```



#### 데이터프레임을 이용한 조인

데이터 프레임을 이용한 조인은 명령어 체인을 이용하여 처리할 수 있습니다.

```scala
val peopleA = sql("select name, age as a_age from people")
val peopleB = sql("select name, age as b_age from people")

scala> peopleA.join(peopleB, "name").select("a_age").show()
+-----+                                                                         
|a_age|
+-----+
|   15|
|   32|
|   40|
|   27|
|   46|
+-----+
```





### 저장

---

데이터를 저장하는 방법은 `write`를 이용합니다. 저정 데이터의 포맷과 저장 모드를 지정하고 저장 위치를 전달하면 데이터를 저장합니다.

`coalesce`를 이용하여 파티션의 개수를 지정하여 파티션의 개수(최종 파일의 개수)를 지정할 수 있습니다.

```scala
import org.apache.spark.sql._

sql("""select * from people""")
.coalesce(1).write.format("json")
.mode(SaveMode.Overwrite).save("/user/people")
```



파일은 다음과 같은 형태로 저장됩니다.

```scala
$ hadoop fs -ls /user/people/
Found 2 items
-rw-r--r--   2 hadoop hadoop          0 2019-01-30 06:04 /user/people/_SUCCESS
-rw-r--r--   2 hadoop hadoop        126 2019-01-30 06:04 /user/people/part-r-00000-d88fccf6-4468-4490-bb91-ba96721fac85.json
```

